\documentclass[aos]{imsart}
\setattribute{journal}{name}{}
\pdfminorversion=4 
\usepackage[left=1in,top=1in,right=1in,bottom=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{hyperref}
\usepackage{graphicx, amsmath, amssymb, fullpage, amsfonts, bbm, bbold}
\usepackage{lineno}
\usepackage{color}
\usepackage[round]{natbib}
\usepackage{enumerate}
\linespread{1.5} 
\newcommand{\abs}[1] {|#1|}
\newcommand{\logit}{\mbox{logit}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\bPsi}{\mathbf{\Psi}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\tepsilon}{\tilde{\epsilon}}
\newcommand{\blambda}{\mathbf{\lambda}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\epsy}{\epsilon}
\newcommand{\epsz}{\nu}
\newcommand{\z}{\mathrm{z}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\probit}{\mbox{probit}}
\newcommand{\hiw}{\mbox{{\small\textsc{HIW}}}}
\newcommand{\iw}{\mbox{{\small\textsc{IW}}}}
\newcommand{\N}{\mbox{{\small\textsc{N}}}}
\newcommand{\T}{\mbox{{\small\textsc{T}}}}
\newcommand{\C}{\mbox{{\small\textsc{C}}}}
\newcommand{\Ga}{\mbox{{\small\textsc{Ga}}}}
\newcommand{\IG}{\mbox{{\small\textsc{IG}}}}
\newcommand{\Be}{\mbox{{\small\textsc{Be}}}}
\newcommand{\Ber}{\mbox{{\small\textsc{Ber}}}}
\newcommand{\dd}{\mbox{d}}
\newcommand{\E}{\mbox{E}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\Tau}{\mathrm{T}}
\newcommand{\x}{\mathrm{x}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\usepackage{bm}

\begin{document}
\begin{frontmatter}
\title{Semi-supervised propensity score estimation}

\author{Andrew Herren} \and \author{P. Richard Hahn}
\affiliation{Arizona State University}
\date{}

\begin{abstract}
We extend the notion of semi-supervised learning to causal inference by considering data as ``unlabeled"
if treatment assignment and covariates are observed but outcomes are unobserved. 
We demonstrate how to make use of unlabeled data in estimating the average treatment effect, 
and validate the results using simulation studies as well as public survey data.
\end{abstract}

%% keywords here, in the form: keyword \sep keyword
\begin{keyword}
Bayesian; Causal inference; Machine learning; Semi-supervised learning; Propensity score; Unlabeled data
\end{keyword}

\end{frontmatter}
% Activate to display a given date or no date
\section{Introduction}
The challenge of inferring causal effects from observational data has led to decades of methodological research.
\cite{rosenbaum1983central} introduce the propensity score as an individual's probability of receiving treatment and show that 
it can produce unbiased estimates of treatment effects when used with matching or stratification estimators.
Propensity score methods have been put to successful use in many domains, including economics (\cite{heckman1997matching}), 
medicine (\cite{luo2010applying}), and political science (\cite{fong2018covariate}).

We show that treatment effect estimates can be improved by using unlabeled treatment assignment 
and covariate data (that is, data points for which outcomes are not observed, but covariates and treatment status are observed). 
With insight from the semi-supervised learning literature, we use the unlabeled data to calculate more accurate propensity 
scores and then use the predicted propensity scores to estimate treatment effects among the labeled observations.

Much work has been done on propensity score methods in the general case of missing data. 
\cite{d2000estimating} extend the ECM algorithm (\cite{meng1993maximum}) to learn propensity scores with missing covariates. 
\cite{williamson2012doubly} introduce doubly robust estimators that can handle missing data in the outcome, treatment, or covariates.
\cite{zhang2016causal} use data with missing treatment assignments to motivate a triply robust estimator of outcome, propensity, and missingness.
The goal of these papers, broadly, is to impute missing data before estimating treatment effects. 

\cite{liang2007use} review the general topic of semi-supervised learning and articulate the conditions 
under which marginal, or unlabeled, data can aid in parameter inference, and when it is unnecessary. 
The semi-supervised learning framework has been applied productively in the machine learning literature 
(see for example, \cite{kingma2014semi}).
\cite{cheng2018efficient} propose a semi-supervised learning framework for estimating average treatment effects. 
However, their method differs from our approach in several ways. 
Most prominently, they aim to build a doubly robust estimator of the ATE, while our aim is to 
learn propensity scores which can be used flexibly in a variety of estimators.

\section{Problem statement and notation}

\subsection{Notation}
\label{subsection:notation}

Consider a dataset with $n$ observations. Let $\Y$ denote the outcome of interest, which we assume to be only partially observed. 
We refer to the data points with observed outcomes as ``labeled data," and let $n_o$ refer to the number of such labeled observations 
and $n_m = n - n_o$ refer to the number of data points with missing outcomes.
Let $\X$ denote covariates, which includes information that may predict treatment assignment, outcomes, or both. 
In many applied settings, covariates include demographics, health markers, or various self-reported behaviors.
$\Z$ refers to a binary treatment assignment. In a controlled experiment, the treatment assignment would be randomized according to a pre-specified design. 
Observational studies are characterized by a lack of direct manipulation of treatment assignment. 
Finally, an outcome's status as missing or observed is indexed by $\M$. Central to our approach is the assumption that outcomes are only 
observed for a fraction of a given dataset.

Following the notation of \cite{liang2007use}, we denote unlabeled, or marginal, data as

$\X^m = \{x_i; i = n_o + 1, ... , n_m \}$, and 

$\Z^m = \{z_i; i = n_o + 1, ... ,n_m \}$, 

where $n_m$ denotes the number of unlabeled covariate data points. We observe that $\M = 1$ where $x \in \X^m, z \in \Z^m$ and $\M = 0$ elsewhere.

\subsection{Average treatment effects and their estimators}

One common goal in causal inference with observational data is to recover an unbiased estimate of the average treatment effect (ATE). Following \cite{rosenbaum1983central}, 
we define $\textrm{ATE} = \E_X[\E[Y | X, Z = 1] - \E[Y | X, Z = 0]]$ as the population expectation of the outcome for those who received treatment less the expected outcome for those who did 
not receive treatment. We assume that $Z$ is exchangeable conditional on $X$. The ATE can be estimated by a number of procedures. This paper will focus on several 
estimators which make use of estimated propensity scores.

The first method we evaluate is the Inverse Propensity Weighted (IPW) estimator of the ATE (\cite{hirano2003efficient}), defined as
\[ \textrm{ATE}_{\textrm{IPW}} = \frac{1}{n_o} \sum_{i = 1}^{n_o} \left( \frac{Y_i Z_i}{\hat{p_i}} - \frac{Y_i (1 - Z_i)}{(1 - \hat{p_i})} \right) \]
where the propensity score, $\hat{p_i} = \hat{p_i}(Z_i = z | X_i)$, is an estimate of the probability of receiving treatment conditional on covariates $X_i$ (\cite{rosenbaum1983central}).

Next, we consider the targeted maximum likelihood estimator (TMLE) (\cite{van2010targeted}, \cite{van2010targeted2}, \cite{gruber2009targeted})
\[\textrm{ATE}_{\textrm{TMLE}} = \frac{1}{n_o} \sum_{i = 1}^{n_o} \left( Q^*\left( Z_i = 1, X_i \right) - Q^*\left( Z_i = 0, X_i \right) \right)\]
where $Q^*\left( Z_i, X_i \right)$ represents a semi-parametric model of the outcome $Y$ which incorporates an estimate of the propensity model.

Finally, we consider Bayesian causal forests (BCF) (\cite{hahn2020bayesian})
\[\textrm{ATE}_{\textrm{BCF}} = \frac{1}{n_o} \sum_{i = 1}^{n_o} \left( \bar{f}(X_i, Z_i = 1) - \bar{f}(X_i, Z_i = 0) \right) \]
where $\bar{f}(X_i, Z_i)$ is an average of posterior simulations (across all simulations) of two BART (\cite{chipman2010bart}) models, defined as $f(X_i, Z_i) = \mu(X_i, \hat{p_i}) + \tau(X_i) Z_i$. $\hat{p_i}$ is an estimate of the propensity score.

Each of these methods is evaluated according to three approaches:

\begin{itemize}
\item \textbf{Complete case analysis}: We discard all unlabeled data samples, estimate $\hat{p}$, and then compute average treatment effects on only the labeled observations.
\item \textbf{Semi-supervised}: We use the labeled and unlabeled $X$ and $Z$ values to estimate $\hat{p}$ and then use the $\hat{p}$ predictions to compute average treatment effects on the labeled observations.
\item \textit{[Simulation only]} \textbf{Ground truth}: Simulation studies provide us with actual probabilities of receiving treatment, so we use these true $p$ to compute average treatment effects on the labeled observations. It is evident that in empirical evaluations tested on real world data, such ground truth will not be available.
\end{itemize}

\section{Empirical results}

Given that most propensity scores model a binary treatment assignment, logistic regression is a natural choice for 
estimating propensity scores, however ~\cite{lee2010improving} point out that the assumptions required for logistic 
regression are not always warranted and examine propensity score estimation using a number of machine learning 
methods, including CART (\cite{breiman1984classification}) and Random Forest (\cite{breiman2001random}).

This paper proceeds similarly, but instead relies on Bayesian Additive Regression Trees (BART) (\cite{chipman2010bart}). 
BART is a nonparametric Bayesian tree ensemble method that learns complex functions via a sum of weak regression 
or classification trees. In the case of our IPW estimator, we also consider a propensity model specified using logistic regression 
in order to investigate how inferences can be harmed with a mis-specified propensity model.

We construct confidence intervals for our simulations as follows:

\begin{itemize}
	\item \textbf{IPW}: We compute the standard error for a logit propensity model as in \cite{cerulli2014treatrew}
	\item \textbf{TMLE}: We use the 95\% confidence interval produced by the \texttt{tmle} R package
	\item \textbf{BCF}: We construct a 95\% credible interval using posterior samples of the treatment effect 
\end{itemize}

\subsection{Evaluation metrics}

We evaluate the results of our simulation using the following metrics.

\begin{itemize}
	\item \textbf{RMSE}: root mean squared error of estimated treatment effects
	\item \textbf{Bias}: difference between true ATE and average estimated ATE
	\item \textbf{Coverage}: share of 95\% confidence intervals that contain the true ATE
\end{itemize}

We will see that some estimators produce biased estimates of the ATE, others produce largely unbiased estimates
but attain poor interval coverage, and in some cases, both phenomena occur. 

\subsection{Simulations}
\label{section:simstudy}

We first test our method using simulated data. We model a number of data generating processes, each of which is designed to 
test the conditions under which propensity scores trained using unlabeled data are helpful or harmful to the ultimate
goal of estimating average treatment effects.

\subsubsection{Data Generating Process}

For simulated covariates, outcomes, and treatment effects, we use a subset of the data generating processes tested in \cite{hahn2020bayesian}. 

\begin{align*}
Y &= \mu(X) + \tau Z + \varepsilon\\
\mu(X) &= 
\begin{cases}
3 + x_1 x_3,&  x_5 = 1,\\
x_1 x_3,&  x_5 = 2\\
-3 + x_1 x_3,&  x_5 = 3
\end{cases} \\
x_1, x_2, x_3 &\sim \mathcal{N}(0, 1)\\
x_4 &\sim \textrm{Bernoulli}(0.5)\\
x_5 &\sim \textrm{Categorical}(0.25, 0.5, 0.25)\\
\tau &= 3\\
\varepsilon &\sim \mathcal{N}(0, 1)
\end{align*}
There are $p = 5$ covariates, the first three of which are independent standard normal variables, the fourth of which is binary, 
and the fifth is an unordered categorical variable with values $1, 2, 3$. Treatment effects are homogeneous ($\tau = 3$), and 
the outcome is determined by a combination of treatment effects and a piecewise interaction function of three of the covariates
($\mu(x) = 1 + g(x_5) + x_1 x_3$, where $g(x) = 2$ if $x = 1$, $g(x) = -1$ if $x = 2$, and $g(x) = -4$ if $x = 3$). 
$\mu(x)$ is often referred to as a \textit{prognostic effect} and can be conceptualized as the expected value of 
the outcome for individuals who do not receive the treatment.
Sample sizes in our simulations vary between $500$, $1000$ and $5000$. 

For treatment assignment, we consider two cases:

\begin{enumerate}
\item $P(Z = 1 | X) = 0.5$
\item $P(Z = 1 | X) = 0.8\Phi(3 \mu(X) / s - 0.5 x_1) + 0.05 + u / 10$
\begin{itemize}
	\item $\Phi(\cdot)$ is the standard normal CDF
	\item $s$ is the sample standard deviation of simulated observation of $\mu(X)$
	\item $u \sim \mathrm{Uniform}(0, 1)$ 
\end{itemize}
\end{enumerate}

The second treatment assignment mechanism is described as ``Targeted Selection" in \cite{hahn2020bayesian}, and 
refers to the simple phenomenon of assigning treatment based on the expected value of the outcome for those who don't receive 
treatment. This could be motivated quite simply as a physician's mental calculus of ``how much worse will these symptoms get if I don't 
assign treatment."

In order to simulate the process by which outcomes are unlabeled, we assume that 90\% of the data is unlabeled and that the status of $\M = 0$ or $\M = 1$
does not correlate with $\Y$, $\X$, or $\Z$. This parallels the notion of missing completely at random (MCAR) (\cite{little2019statistical}) in the missing data 
literature, though it is worth noting that our problem is not a traditional missing data problem. We treat our unlabeled $X^m$ and $Z^m$ as auxiliary data that 
may be helpful in estimating the ATE rather than treating our unobserved $Y^m$ as missing data to be imputed or otherwise estimated.

\subsubsection{Simulation results under targeted selection}

We first summarize the key takeaways from our simulations before proceeding to a more detailed discussion. 

\begin{itemize}
	\item Using unlabeled data will harm inferences in the case of a mis-specified propensity model, as confidence intervals are narrow around a biased estimate
	\item BCF and IPW/BART both work best in the targeted selection scenario
	\item While randomized treatment assignment makes ATE estimation much easier, unlabeled data can still be useful in this case for variance reduction purposes
\end{itemize}

We first examine the targeted selection case, in which treatment assignment is correlated with the 
prognostic effect. The table below shows the results for 500 simulations using an IPW estimator and propensities 
estimated by logistic regression. Logistic regression cannot capture the complexity of the simulated 
treatment assignment in the targeted selected scenario, and we see that all of the results 
which use estimated propensities are badly biased. Furthermore, we observe that using unlabeled data 
and increasing sample size is harmful to proper inference of the true ATE, as the variance is reduced 
around a biased estimate, leading to poor interval coverage.

\input{../../outputs/simulations_IPW_LIN_HOM_MCAR_90.tex}

The table below shows the results for 500 simulations using an IPW estimator and propensities 
estimated by BART. In this case, the propensity model is not mis-specified, and we observe that 
the use of unlabeled data reduces bias and increases interval coverage.

\input{../../outputs/simulations_IPW_BART_LIN_HOM_MCAR_90.tex}

The table below shows the results for 500 simulations using a TMLE estimator and propensities 
estimated by BART. In this case, the propensity model is not mis-specified, however, we observe 
that the TMLE estimator achieves poor interval coverage, even in high sample sizes when the 
average bias is lower.

\input{../../outputs/simulations_TMLE_LIN_HOM_MCAR_90.tex}

The table below shows the results for 500 simulations using a BCF estimator and propensities 
estimated by BART. BCF was designed in part to handle the case of targeted selection, and we 
see that it produces unbiased estimates with high coverage as more unlabeled data is incorporated.

\input{../../outputs/simulations_BCF_LIN_HOM_MCAR_90.tex}

\subsubsection{Simulation results under randomized assignment}

We now turn to the randomized treatment assignment scenario. 
The table below shows the results for 500 simulations using the Complete Case approach 
across all four estimators. We see in this case that all methods produce unbiased estimates 
of the ATE as sample size increases, and with the exception of the TMLE estimator, the approaches 
all achieve high coverage of the true treatment effect.

\input{../../outputs/simulations_CC_ALL_EXP_LIN_HOM_MCAR_90.tex}

We now test the same simulations using the semi-supervised approach, and observe that 
each method produces unbiased estimates of the ATE with larger sample sizes, but that using 
unlabeled data reduces the variance of the IPW estimates, so that coverage is closer to 95\%.

\input{../../outputs/simulations_SSL_ALL_EXP_LIN_HOM_MCAR_90.tex}

\subsection{Real Data Example}

\subsubsection{NHEFS}

Following \cite{hernan2020causal}, we use data from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS) 
to estimate the effect of quitting smoking on weight gain. 
We use a modified version of the NHEFS data available on the book website for \cite{hernan2020causal}\footnote{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}. 
Following the discussion in Chapter 12, we remove missing values for any of the data fields which will be used in estimating the ATE: age, sex, race, weight in 1971, weight in 1982, height, 
education level, frequency of alcohol consumption, number of daily cigarettes smoked in 1971, and the change in number of daily cigarettes smoked between 1971 and 1982.

After making these adjustments, we summarize the differences in the treatment and control groups across a number of variables and see that the results closely match those of \cite{hernan2020causal}.

\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{lllrrr}
  \hline
Treatment Status & Age & \% Men & \% White & Weight (kg), 1971 & Cigarettes per day, 1971 \\ 
  \hline
Quit smoking & 46.2 & 54.6 & 91.1 & 72.4 & 18.6 \\ 
Have not quit smoking & 42.8 & 46.6 & 85.4 & 70.3 & 21.2 \\ 
   \hline
\end{tabular}
\endgroup
\caption{NHEFS - summary of covariates by treatment group} 
\end{table}

Given the two approaches which showed the strongest finite sample properties in our simulation studies were IPW-BART and BCF, we use 
both methods to estimate the average effect on weight gain of quitting smoking.
In each case, we use the same eleven potential confounders as covariates in a propensity model and estimate the overall ATE either via IPW or BCF.
With IPW-BART, we estimate an average treatment effect of 3.0 kilograms with a 95\% confidence interval of $(2.0, 3.9)$ and, with BCF we estimate an 
average treatment effect of 3.4 kilograms and a 95\% credible interval of $(2.5, 5.9)$.

To explore the potential use of unlabeled data, we proceed by removing data at random from this dataset, estimating complete case and semi-supervised ATEs, and repeating the procedure 
on many sub-samples of the data. Table 8 below reports the average treatment effects estimated at different levels of outcome missingness.
These results highlight the same trends seen in our simulation studies. Using unlabeled data to estimate propensity scores generally improves the quality of our estimates, but having an 
adequate sample of observed outcomes is crucial. 80\% of outcomes missing translates to roughly 300 observed outcomes, and we see that in this case the semi-supervised approach comes 
much closer to approximating the full data ATE.

\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{ccc}
  \hline
\% Outcomes Missing & Complete Case & Semi-Supervised \\ 
  \hline
80\% & 2.5 & 3.0 \\ 
50\% & 2.8 & 3.0 \\ 
20\% & 2.9 & 3.0 \\ 
   \hline
\end{tabular}
\endgroup
\caption{IPW-BART Comparison of Complete Case and Semi-Supervised Approaches (200 sub-samples)} 
\end{table}

In running this experiment with the BCF estimator, we see that the difference between the semi-supervised and complete case approaches is virtually indistinguishable. 

\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{ccccc}
  \hline
\% Outcomes Missing & Complete Case & Semi-Supervised \\ 
  \hline
80\% & 3.0 & 3.0 \\ 
50\% & 3.3 & 3.4 \\ 
20\% & 3.4 & 3.4 \\ 
   \hline
\end{tabular}
\endgroup
\caption{BCF Comparison of Complete Case and Semi-Supervised Approaches (200 sub-samples)} 
\end{table}

\newpage
\section{Discussion}

\subsection{Theoretical motivation of semi-supervised propensity score results}
\label{subsection:theoretical}

To understand when and to what extent unlabeled data can be useful in estimating average treatment effects with propensity scores, we consider a theoretical example. 
Let $Y$ be an outcome of interest, $X$ be a set of covariates, and $Z$ be treatment assignment. Following \cite{hernan2020causal}, we define the ATE according to: 
$\E[Y | Z] = \theta_1 + \theta_2 Z$, where $\theta_2$ is the ATE estimand for a binary treatment variable $Z$. 
We write the two parameters of this model as $\theta = [\theta_1, \textrm{ } \theta_2]$ and the matrix $U = [1, \textrm{ } Z]$ as the treatment assignments with a vector of 
ones appended. Thus, we can formulate the conditional expected outcome as $\E[Y | Z] = U\theta$. 
As \cite{hernan2020causal} note, as long as every individual has a non-zero probability of receiving treatment, the IPW estimator of the ATE is equivalent in expectation to 
the weighted least squares estimator of $\theta_2$, given by the second element of the vector $\hat{\theta} = \left(U^TWU\right)^{-1}U^TWY$, where $W$ is a 
diagonal matrix of weights computed from estimated propensity scores ($\hat{p}_i$) as follows:
\begin{equation*}
\begin{aligned}
w_i & = 
\begin{cases}
\frac{1}{\hat{p}_i},&  Z_i = 1,\\
\frac{1}{1 - \hat{p}_i},&  Z_i = 0,
\end{cases} \\
\end{aligned}
\end{equation*}

\cite{gelman2006data} point out that the weighted least squares estimator is the maximum likelihood estimator for 
$\theta$ in the data model $Y_i \sim \Norm(Z_i\theta, \frac{\sigma^2}{w_i})$. Consider the following data generating process:

\begin{equation*}
\begin{aligned}
X & \sim \Norm(0, \Sigma_1)\\
\beta & \sim \Norm(0, \Sigma_2)\\
\theta & \sim \Norm(0, \Sigma_3)\\
p_i & = \left(\frac{1}{1+\exp(-X_i\beta)}\right)\\
Z_i | p_i & \sim \textrm{Bernoulli}(p_i)\\
w_i & = 
\begin{cases}
\frac{1}{p_i},&  Z_i = 1,\\
\frac{1}{1 - p_i},&  Z_i = 0,
\end{cases} \\
Y_i | Z_i, \theta & \sim \Norm(Z_i\theta, \frac{\sigma^2}{w_i})\\
\end{aligned}
\end{equation*}

As in Section \ref{subsection:notation}, we assume that, in addition to $Y$, $Z$, and $X$, we also have data $Z^m$ and $X^m$ for which no outcomes are observed. 
We are interested in characterizing the posterior distribution $p(\theta, \beta | Y, Z, X)$ to determine whether information contained in $Z^m$ and $X^m$ will be helpful 
in making inferences about $\theta$. We can see that 
\begin{equation*}
\begin{aligned}
p(\theta, \beta | Y, Z, X) & \propto p(Y | Z, \theta, w, \sigma) p(Z | X, \beta) p(X | \Sigma_1) p(\theta | \Sigma_2) p(\beta | \Sigma_3)\\
\end{aligned}
\end{equation*}
The unlabeled data enter this posterior via the propensity model $p(Z | X, \beta)$ and the covariate likelihood $p(X | \Sigma_1)$.
We see that when the true propensities are unknown and must be estimated from the data, the marginal data $Z^m$ and $X^m$ are useful in inferring $\theta$. 
This is driven by the fact that the weights in the outcome likelihood depend on $\beta$ which is learned from the labeled and unlabeled samples of ($X, Z$).

However, when $p(Z | X, \beta) = p$ (i.e. $Z \perp X$) for some known value $p$, as in the case of a randomized experiment, the weights $w$ in the outcome 
likelihood will be constant in $X$, so the posterior for $\theta$ does not depend on $\beta$.
In this case, unlabeled data $Z^m$ and $X^m$ are irrelevant in making inferences about $\theta$, because $p(Y | Z, \theta, w, \sigma)$ is only informed by the known 
probability $p$. This is also true of the more general case in which $Z \not\perp X$ but the true treatment probabilities are known for each value of $X$.

\subsection{Estimated versus true propensities}
\label{subsection:estimatedprop}

A recurrent theme in the causal inference literature is that treatment effect estimators which rely on fitted propensities attain a lower variance than estimators that 
rely on known, true propensities. Taken at face value, this point would appear to invalidate our recommendation to use unlabeled data in treatment effect estimation. 
Why use unlabeled data to better approximate the true propensities, when the estimated propensities lead to superior inference? 
What appears at first glance as a conflict between our approach and the received statistical wisdom masks a series of interesting and subtle points. We feel that this topic 
deserves a more detailed and consolidated elucidation, which we present in the following section.

\cite{rosenbaum1987model} provides the first detailed treatment of this question in the context of estimating population outcomes from sample data. 
Consider a population of interest in which each of $S$ strata have (possibly different) probabilities of selection. He compares three estimators of a population outcome:
\begin{enumerate}
\item Empirical postratification: each stratum is weighted by its proportion of sampled units
\item Model-based postratification: each stratum is weighted by a parametric estimate of its sampling probability
\item Known weight postratification: each stratum is weighted by its known sampling probability
\end{enumerate} 
He shows that, conditional on the observed sample, estimators 1 and 2 are unbiased, but 3 is biased. Citing \cite{holt1979post}, he notes that post-stratification estimators 
which are unbiased conditional on an observed sample are preferable.

\cite{imbens1996efficient} develop an efficient estimator for stratified sampling data which necessitates a moment constraint on the selection probability less the observed 
selection variable. When the true selection probabilities are known, they note that this approach is similar in nature to the Seemingly Unrelated Regression (SUR) 
(\cite{zellner1963estimators}), in which models fit simultaneously can experience efficiency gains even when there are no common parameters. 
\cite{hirano2003efficient} also show that estimators which use the true propensities can attain the efficiency bound by incorporating additional moment constraints on 
the selection (or treatment) variable, covariates, and the true propensities. \cite{lunceford2004stratification} demonstrate that the IPW estimator 
with estimated propensities has a lower large sample variance than the same estimator using true propensities. 

To address these points and reconcile them to our approach, we note first and foremost that the results are primarily concerned with the asymptotic behavior of estimators. 
It is certainly true in practice that the sample sizes required to obtain a minimum-variance treatment effect estimate depends on a number of factors, so that one can never 
be sure an asymptotic result holds for their particular problem. 
Indeed, our simulation results show that, for finite samples, using the true propensities (or more accurate estimates of the true propensities using unlabeled data) can lead 
to reduced bias and greater interval coverage. Put differently, without enough data to attain asymptotic results, the risk of bias greatly outweighs the risk of inefficiency.

Second, we note that the results apply to weighting, stratification, and matching estimators. There are many practical alternatives to these methods, such as 
BCF, which do not exhibit this behavior. Third, this phenomenon depends entirely upon being able to construct a propensity model that is an unbiased estimate of the true 
propensities. Any amount of mis-specification (discussed the next section) will completely invalidate the efficiency gains by yielding a biased estimate of the average 
treatment effect.

Finally, we demonstrate a modification to the weighting estimator which uses the true propensities and attains a lower variance than the fitted-propensity estimator.
Consider $X$, $Z$, and $Y$ as covariates, treatment assignment, and outcome, and $p$ as the true probability of receiving treatment. Inverting the logit function, we transform the true 
propensities into $h = - \log(\frac{1}{p} - 1)$ and fit a logistic regression of $Z$ on $h$. Using the predicted probabilities from this regression model as weights in an IPW estimator, 
we see in simulation studies that this method attains the lowest variance among three alternatives:
\begin{enumerate}
\item True propensities as weights
\item Estimated propensities as weights
\item Modified true propensities as weights
\end{enumerate} 

In summary, we advise researchers who have access to true propensities, or a set of unlabeled data that will help in approximating the true propensities, to use that data in 
estimating treatment effects. Researchers who wish to use a weighting estimator can simply modify the true propensities to reduce the variance of their estimates, and researchers 
using more flexible techniques such as BCF should not worry about this result in any case.

\subsection{Propensity model specification}
\label{subsection:overfitprop}

Though there may be benefits to using estimated propensity scores, in practice, calculating propensity scores introduces a new set of risks. The simulations in Section \ref{section:simstudy} 
show that a misspecified propensity model can bias estimates, sometimes drastically. It may seem natural, then, to treat a propensity model as any other supervised learning 
problem, with variable selection, model validation, and diagnostics to ensure a proper fit. 
Indeed, \cite{mccaffrey2004propensity} use generalized boosted models (GBM) to estimate propensities, in part for the automatic variable selection and flexible model specification that 
tree ensembles provide. Their results show that using GBM for a propensity model leads to better covariate balance and lower standard errors than a logit propensity model.

However, \cite{hernan2020causal} caution that traditional model selection techniques may 
be of dubious utility in constructing propensity models. They note that the purpose of the propensity score is to construct an alternate population in which exchangeability is not 
conditional on any of the measured covariates. They are careful to point out that a purely data-driven approach to model selection runs the risk of including variables, such as 
colliders, mediators, or post-treatment variables, that jeopardize identification of treatment effects. They also advise that, even when treatment effects are identified, including 
variables which are not confounders, but are strongly associated with treatment assignment, in a propensity model can increase the variance of treatment effect estimates 
by pushing estimated propensity weights closer to 0 and 1.

While it is certainly true that propensity models can be incorrect in ways that make estimation of treatment effects impossible or numerically unstable, a number of practical steps 
can be taken to mitigate these concerns. First, estimated propensity scores should always be visualized or otherwise inspected to ensure that they are 
not numerically close to 0 and 1. Second, methods like ridge regression or tree ensembles, which introduce some measure of regularization, can be used instead of logistic regression 
to minimize the risk of estimating propensity scores close to 0 and 1. Finally, subject matter expertise should always play a role in building propensity models. However, we are aware that 
for many applied problems, especially those with a high-dimensional covariate set, the challenge of constructing an accurate causal graph is highly non-trivial. Insofar as subject-matter 
expertise can identify variables such as colliders, post-treatment variables, or non-confounders, such variables should of course be excluded from any propensity model. 
Researchers who are concerned about an underlying causal graph invalidating their propensity model can also turn to the causal discovery literature (i.e. \cite{peters2017elements}). Specifically, 
\cite{pmlr-v31-entner13a} present an approach to the problem of identifying confounders for adjustment in a causal model. In practice, the validity of assumptions about a causal graph 
are often difficult to assess, and many researchers will have to assume that their study design (whether observational or experimental) is valid and be open about their assumptions. 

In order to better understand the risk of including non-confounders in a propensity model, we test several possibilities in a simulation study. Letting $X_j$ be a variable in a set of $p$ covariates, 
we consider four cases:
\begin{enumerate}[a)]
\item \textbf{Case 1}: $X_j$ is a confounder and is included in a propensity model
\item \textbf{Case 2}: $X_j$ is \underline{\textbf{not}} a confounder and is included in a propensity model  
\item \textbf{Case 3}: $X_j$ is a confounder and is \underline{\textbf{not}} included in a propensity model
\item \textbf{Case 4}: $X_j$ is \underline{\textbf{not}} a confounder and is \underline{\textbf{not}} included in a propensity model
\end{enumerate}
Cases 1 and 4 are ideal, while cases 2 and 3 both fail to properly account for the true underlying causal model. The question we are concerned with is whether erring on the side 
of including non-confounders in a propensity model is as harmful to inference as ignoring true confounders. To do this, we simulate 100 datasets of $n = $1,000 from the following data generating process:
\begin{align*}
Y &= X\beta + \tau Z + \varepsilon\\
X, \varepsilon &\sim \mathcal{N}(0, 1)\\
\tau &= 3\\
p & = \left(\frac{1}{1+\exp(-X\gamma)}\right)\\
Z & \sim \textrm{Bernoulli}(p)
\end{align*}
For all variables except $X_j$, the $\beta$ and $\gamma$ coefficients for the outcome and propensity models are drawn uniformly at random from a range of $(-0.5, 0.5)$ and $(-0.3, 0.3)$, 
respectively. For the scenarios in which $X_j$ is a confounder (that is, $X_j$ impacts both $Y$ and $Z$), we set $\beta_{X_j} = 0.5$.
Each of our simulations assume some relationship between $X_j$ and $Z$, and we vary $\gamma_{X_j}$ between $\{0, 1, 10\}$ to test the extent to which conditioning on $X_j$ in a 
propensity model separates the treatment and control groups.

Below we see the simulated bias, RMSE, and 95\% interval coverage of each of the four cases, using BCF with a BART propensity model and $\gamma_{X_j} = 1$

\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{cccccc}
  \hline
Case & $X_j$ confounder? & $X_j$ in $\hat{p}$? & Bias & RMSE & Coverage \\ 
  \hline
1 & Yes & Yes & 0.004 & 0.098 & 89\% \\ 
2 & No & Yes & -0.008 & 0.096 & 88\% \\ 
3 & Yes & No & 0.068 & 0.109 & 84\% \\ 
4 & No & No & -0.007 & 0.087 & 87\% \\ 
   \hline
\end{tabular}
\endgroup
\caption{ATE estimation using BCF, $\gamma_{X_j} = 1$} 
\end{table}
We see little difference in the results across each of the cases, because the influence of $X_j$ on $Y$ is modest in the confounded case. 
Below is the same set of outputs for $\gamma_{X_j} = 10$.

\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{cccccc}
  \hline
Case & $X_j$ confounder? & $X_j$ in $\hat{p}$? & Bias & RMSE & Coverage \\ 
  \hline
1 & Yes & Yes & 0.033 & 0.253 & 91\% \\ 
2 & No & Yes & -0.09 & 0.262 & 87\% \\ 
3 & Yes & No & 0.24 & 0.252 & 80\% \\ 
4 & No & No & -0.045 & 0.126 & 98\% \\ 
   \hline
\end{tabular}
\endgroup
\caption{ATE estimation using BCF, $\gamma_{X_j} = 10$} 
\end{table}

We see that cases 1 and 4 exhibit the best performance (since they properly incorporate the true 
causal graph), but even when $X_j$ and $Z$ are strongly related, conditioning on $X_j$ in a propensity mode 
when $X_j$ is not a confounder is less harmful than excluding $X_j$ when it is a confounder. 
To understand why, we briefly review the discussion of ``regularization-induced confounding" (RIC) covered in \cite{hahn2020bayesian}.

As we have seen in our simulations, classic parametric models of a treatment assignment are vulnerable to 
misspecification. While this shortcoming suggests the use of flexible machine learning methods to estimate propensity scores, such methods run the risk of overfitting the data and estimating propensities close to 0 or 1. 
In addition to numeric instability of common ATE estimators, such overfitting can also violate conditional exchangeability. 
Researchers who want to fit flexible propensity models while avoiding overfitting can use regularized machine learning methods, such as BART. \cite{hahn2020bayesian}, building on \cite{hahn2018regularization}, show that a naive regularized model can bias treatment effect estimates. 
Their proposed solution, BCF, controls this bias by incorporating the estimated propensities as a feature in a regression model predicting $\E(Y \mid Z = 0, \hat{p}, x)$.

In our case, since the simulation studies above are conducted using BCF with a BART propensity model, we avoid the problem of an overfit propensity model separating treated and control groups while also recovering unbiased estimates of the average treatment effect. 
This explains why including a non-confounder, $X_j$, which is strongly predictive of $Z$ in the propensity model doesn't harm inference of the ATE by nearly as much as excluding $X_j$ when it is a 
confounder. 

\subsection{Connections to manifold learning theory}

\cite{belkin2006manifold} demonstrate a technique for semi-supervised learning which learns a low-dimensional representation of the feature space and incorporates that representation 
as a penalty term in a loss function. \cite{bickel2007local} show that a sufficiently flexible locally-weighted regression will tend to rely on the relevant dimensions of the feature space 
without making any explicit effort to learn a representation of the low(er) dimensional space. While these results may appear contradictory, it is worth noting that they have slightly different goals. 
\cite{bickel2007local} show that in a supervised learning problem, a low-dimensional representation does not need to be explicitly learned in order to be incorporated into a regression function. 
Their approach does not provide any method for making use of unlabeled data, however, while \cite{belkin2006manifold} explicitly aim to introduce unlabeled data in their learning problem.

The connection between propensity score methods and manifold learning is subtle. In manifold learning, the objective is to constrain a fitted function such that samples which are close 
in the underlying manifold are also close in their predictions. In the context of treatment effect estimation, \cite{hahn2020bayesian} note that the propensity score can be viewed 
as a one-dimensional representation of the feature space. Propensity scores could thus be considered as a constraint which ensures that samples which are close in the propensity space 
will differ on average by treatment assignment and the associated causal effect. In either case, the key results of \cite{belkin2006manifold} show that, if a low dimensional representation is 
desired for an estimation problem, using unlabeled data (from the same underlying distribution) can yield better estimates.

\subsection{Parallels to survey inference}

The use of auxiliary data in survey population inference has a rich history. \cite{horvitz1952generalization} 
introduce estimators of survey totals based on measured quantities from a survey and auxiliary information about the population of interest.
\cite{GelmanLittle97} develop multilevel regression and post-stratification (MRP), 
in which a multilevel model is used to estimate sample parameters for various demographic strata and population data 
(often from public sources such as the census) is used to adjust those parameters to reflect their distribution in the population. 
\cite{WANG2015980} show that a convenience sample with large sample size can be used along with 
high quality auxiliary population data to accurately forecast election outcomes using MRP. 
As response rates decline for traditional probability surveys (\cite{kennedy2019response}), 
practitioners will be forced to rely increasingly on non-probability surveys.
\cite{mkks2017} have drawn connections between observational causal inference and non-probability surveys. 
The semi-supervised propensity score method extends quite naturally to non-probability survey estimation 
by treating auxiliary data (from federal surveys, the Census Bureau, etc...) as ``unlabeled."

\subsection{Acknowledgments}

This work was partially supported by NSF Award \#1502640.

\bibliographystyle{imsart-nameyear} 
\bibliography{semi-supervised-propensity}

%\appendix 
%\section{}

\end{document}  